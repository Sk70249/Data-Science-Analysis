{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\n",
    "sell_prices = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\n",
    "sales_train = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\n",
    "sample_sub = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_reduction(dataset):\n",
    "    column_types = dataset.dtypes\n",
    "    temp = None\n",
    "    for x in range(len(column_types)):\n",
    "        column_types[x] = str(column_types[x])\n",
    "    for x in range(len(column_types)):\n",
    "        temp = dataset.columns[x]\n",
    "        if dataset.columns[x] == \"date\":\n",
    "            dataset[temp] = dataset[temp].astype(\"datetime64\")\n",
    "        if column_types[x] == \"int64\" and dataset.columns[x] != \"date\":\n",
    "            dataset[temp] = dataset[temp].astype(\"int16\")\n",
    "        if column_types[x] == \"object\" and dataset.columns[x] != \"date\":\n",
    "            dataset[temp] = dataset[temp].astype(\"category\")\n",
    "        if column_types[x] == \"float64\" and dataset.columns[x] != \"date\":\n",
    "            dataset[temp] = dataset[temp].astype(\"float16\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist priority is to reduce the memory of the data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = memory_reduction(calendar)\n",
    "sell_prices = memory_reduction(sell_prices)\n",
    "sales_train = memory_reduction(sales_train)\n",
    "sample_sub = memory_reduction(sample_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA WRANGLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets start with CALENDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we arent removing data for the moment like we did previously \n",
    "# Next we need to transform our dates to usable or model efficient formats\n",
    "# For that we will be creating a day column and week num columns and reducing there memory\n",
    "calendar[\"day\"] = pd.DatetimeIndex(calendar[\"date\"]).day\n",
    "calendar[\"day\"] = calendar[\"day\"].astype(\"int8\")\n",
    "calendar[\"week_num\"] = (calendar[\"day\"] - 1) // 7 + 1\n",
    "calendar[\"week_num\"] = calendar[\"week_num\"].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets see it \n",
    "calendar.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to add a category named missing in order to omit the NaN values as we have described object data type to category data type\n",
    "calendar[\"event_name_1\"] = calendar[\"event_name_1\"].cat.add_categories('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# we are droping date\n",
    "# we are stripping d_ values to int\n",
    "# changing Nan values to mising in events \n",
    "# and also integer encoding the values of catagorical variable\n",
    "def prep_calendar(df):\n",
    "    df = df.drop([\"date\", \"weekday\"], axis=1)\n",
    "    df = df.assign(d = df.d.str[2:].astype(int))\n",
    "    df[\"event_type_1\"] = df[\"event_type_1\"].cat.add_categories('missing')\n",
    "    df[\"event_name_2\"] = df[\"event_name_2\"].cat.add_categories('missing')\n",
    "    df[\"event_type_2\"] = df[\"event_type_2\"].cat.add_categories('missing')\n",
    "    \n",
    "    df = df.fillna(\"missing\")\n",
    "    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n",
    "    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n",
    "    df = memory_reduction(df)\n",
    "    return df\n",
    "\n",
    "calendar = prep_calendar(calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be configuring SALES data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically changing sales data from wide to long format \n",
    "\n",
    "\n",
    "def reshape_sales(df, drop_d = None):\n",
    "    if drop_d is not None:\n",
    "        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n",
    "    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n",
    "    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n",
    "    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "                 var_name='d', value_name='demand')\n",
    "    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n",
    "    return df\n",
    "\n",
    "sales_train = reshape_sales(sales_train, 488)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating embeddings for catagorcal variables rather than using dummies **please change it if you want too **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for that we are using ordinal encoder from sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are calculating rolling mean and standard deviation\n",
    "def prep_sales(df):\n",
    "    #this is shifting the data by 28\n",
    "    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n",
    "    #rolling mean window 7\n",
    "    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "    #rolling mean window 15\n",
    "    df['rolling_mean_t15'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(15).mean())\n",
    "    #rolling mean window 30\n",
    "    df['rolling_mean_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "    #rolling std window 7\n",
    "    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "    #rolling mean window 30\n",
    "    df['rolling_std_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "\n",
    "    # Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n",
    "    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t30))]\n",
    "    df = memory_reduction(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "sales_train = prep_sales(sales_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW lets configure our sellin price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have only calculated rolling mean, std , cummulative relation btween them \n",
    "def prep_selling_prices(df):\n",
    "    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n",
    "    df[\"sell_price_rel_diff\"] = gr.pct_change()\n",
    "    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n",
    "    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) / (1 + gr.cummax() - gr.cummin())\n",
    "    df = memory_reduction(df)\n",
    "    return df\n",
    "\n",
    "sell_prices = prep_selling_prices(sell_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging with calendar\n",
    "sales_train = sales_train.merge(calendar, how=\"left\", on=\"d\")\n",
    "gc.collect()\n",
    "sales_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train = sales_train.merge(sell_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\n",
    "sales_train.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "gc.collect()\n",
    "sales_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sell_prices\n",
    "gc.collect()\n",
    "sales_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\n",
    "cat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n",
    "                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "# if you want to check the progress of it use \n",
    "# from tqdm import tqdm\n",
    "# for i, v in tqdm(enumerate(cat_id_cols)):\n",
    "# In loop to minimize memory use\n",
    "for i, v in enumerate(cat_id_cols):\n",
    "    sales_train[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales_train[[v]])\n",
    "\n",
    "sales_train = memory_reduction(sales_train)\n",
    "gc.collect()\n",
    "sales_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n",
    "            \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t15\", \"rolling_mean_t30\", \n",
    "            \"rolling_std_t7\", \"rolling_std_t30\"]\n",
    "bool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\n",
    "dense_cols = num_cols + bool_cols\n",
    "\n",
    "# Need to do column by column due to memory constraints\n",
    "for i, v in enumerate(num_cols):\n",
    "    sales_train[v] = sales_train[v].fillna(sales_train[v].median())\n",
    "    \n",
    "sales_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sales_train[sales_train.d >= 1914]\n",
    "test[\"id\"] = test[\"id\"].astype(\"str\")\n",
    "test = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n",
    "                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\n",
    "test.head()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dict for training with a dense array and separate inputs for each embedding input\n",
    "def make_X(df):\n",
    "    X = {\"dense1\": df[dense_cols].to_numpy()}\n",
    "    for i, v in enumerate(cat_cols):\n",
    "        X[v] = df[[v]].to_numpy()\n",
    "    return X\n",
    "\n",
    "# Submission data\n",
    "X_test = make_X(test)\n",
    "\n",
    "# One month of validation data\n",
    "flag = (sales_train.d < 1914) & (sales_train.d >= 1914 - 28)\n",
    "valid = (make_X(sales_train[flag]),\n",
    "         sales_train[\"demand\"][flag])\n",
    "\n",
    "# Rest is used for training\n",
    "flag = sales_train.d < 1914 - 28\n",
    "X_train = make_X(sales_train[flag])\n",
    "y_train = sales_train[\"demand\"][flag]\n",
    "                             \n",
    "del sales_train, flag\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
